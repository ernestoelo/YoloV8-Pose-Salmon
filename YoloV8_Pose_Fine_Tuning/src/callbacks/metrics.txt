"""
Callbacks personalizados para YOLOv8
"""
import yaml
from pathlib import Path
import matplotlib.pyplot as plt
import numpy as np
import torch
import pandas as pd
from src.metrics.evaluator import PoseEvaluator


class CustomMetricsCallback:
    """Callback para calcular m√©tricas personalizadas durante entrenamiento"""
    
    def __init__(self, config_path: str):
        """
        Args:
            config_path: Ruta al archivo de configuraci√≥n
        """
        with open(config_path, 'r') as f:
            self.config = yaml.safe_load(f)
        
        self.evaluator = PoseEvaluator(self.config)
        self.batch_metrics = []
        
    def on_val_batch_end(self, validator):
        """
        Ejecutado al final de cada batch de validaci√≥n.
        Aqu√≠ es donde 'interceptamos' los resultados parciales para calcular nuestras m√©tricas.
        """
        try:
            # Inicializar flag de debug si no existe
            if not hasattr(self, 'debug_printed'):
                self.debug_printed = False

            # El objeto 'batch' contiene las im√°genes y etiquetas reales (Ground Truth)
            batch = validator.batch
            if batch is None:
                if not self.debug_printed:
                    print("DEBUG: validator.batch es None")
                    self.debug_printed = True
                return

            # 1. Determinar el tama√±o del batch actual
            # batch['keypoints'] es un Tensor de PyTorch con forma [Batch_Size, Num_Keypoints, 3]
            gt_kpts_batch = batch['keypoints'].detach().cpu().numpy()
            batch_size = gt_kpts_batch.shape[0]

            # 2. Buscar el contenedor de predicciones (puede cambiar de nombre seg√∫n versi√≥n)
            preds_container = None
            if hasattr(validator, 'preds'):
                preds_container = validator.preds
            elif hasattr(validator, 'predictions'):
                preds_container = validator.predictions
            
            # Si no encontramos el contenedor
            if preds_container is None:
                if not self.debug_printed:
                    print("DEBUG: No se encontr√≥ 'preds' ni 'predictions' en el validator.")
                    # Imprimir atributos disponibles para diagnosticar
                    attrs = [a for a in dir(validator) if not a.startswith('__')]
                    print(f"DEBUG: Atributos disponibles: {attrs}")
                    self.debug_printed = True
                return
            
            if not preds_container:
                if not self.debug_printed:
                    print("DEBUG: El contenedor de predicciones est√° vac√≠o.")
                    self.debug_printed = True
                return

            # 3. Recuperar las predicciones correspondientes a este batch
            if len(preds_container) < batch_size:
                if not self.debug_printed:
                    print(f"DEBUG: Menos predicciones ({len(preds_container)}) que tama√±o de batch ({batch_size})")
                    self.debug_printed = True
                return 
            
            current_preds = preds_container[-batch_size:]

            # 3. Extraer keypoints de los objetos Results de YOLO
            pred_kpts_list = []
            for result in current_preds:
                # result.keypoints.data suele ser un Tensor [1, K, 3] (x, y, confianza)
                if hasattr(result, 'keypoints') and result.keypoints is not None:
                    kpts = result.keypoints.data.cpu().numpy()
                    # A veces viene sin la dimensi√≥n del batch [K, 3], as√≠ que la agregamos
                    if len(kpts.shape) == 2: 
                        kpts = np.expand_dims(kpts, axis=0)
                    pred_kpts_list.append(kpts)
                else:
                    # Si YOLO no detect√≥ nada en la imagen, rellenamos con ceros
                    # para mantener la consistencia en los arrays
                    pred_kpts_list.append(np.zeros((1, gt_kpts_batch.shape[1], 3)))

            # Concatenar toda la lista en un solo array gigante [Batch_Size, K, 3]
            if pred_kpts_list:
                pred_kpts_batch = np.concatenate(pred_kpts_list, axis=0)
            else:
                return

            # 4. Preparar diccionarios para el evaluador
            # El evaluador espera arrays de NumPy limpios
            predictions = {
                'keypoints': pred_kpts_batch, 
                'bboxes': batch['bboxes'].detach().cpu().numpy() if 'bboxes' in batch else None
            }
            
            ground_truth = {
                'keypoints': gt_kpts_batch[..., :2], # Tomamos solo x,y [N, K, 2]
                'visibilities': gt_kpts_batch[..., 2] # La tercera columna es la visibilidad [N, K]
            }
            
            # Calcular m√©tricas de este batch espec√≠fico y guardarlas
            metrics = self.evaluator.evaluate_batch(predictions, ground_truth)
            self.batch_metrics.append(metrics)
            
        except Exception as e:
            # Si algo falla en nuestro c√≥digo personalizado, no queremos romper el entrenamiento de YOLO.
            # Solo imprimimos el error la primera vez y continuamos.
            if len(self.batch_metrics) == 0:
                print(f"‚ö†Ô∏è Advertencia en CustomMetricsCallback: {e}")

    def on_val_end(self, validator):
        """Ejecutado al final de cada validaci√≥n"""
        if not self.batch_metrics:
            print("‚ö†Ô∏è No se calcularon m√©tricas personalizadas (lista vac√≠a).")
            return

        print("\nüîç Calculando m√©tricas personalizadas...")
        
        # Promediar m√©tricas de todos los batches
        df_batch = pd.DataFrame(self.batch_metrics)
        avg_metrics = df_batch.mean().to_dict()
        
        # Agregar resultado al evaluador
        self.evaluator.add_result(validator.epoch, avg_metrics)
        
        # Mostrar resumen en consola
        print(f"   üìä √âpoca {validator.epoch} - Resumen:")
        for k, v in avg_metrics.items():
            if 'pck' in k or 'oks' in k: # Mostrar solo las principales
                print(f"      ‚Ä¢ {k}: {v:.4f}")
        
        # Guardar CSV actualizado
        save_path = Path(validator.save_dir) / 'custom_metrics.csv'
        self.evaluator.save(str(save_path))
        
        # Limpiar para la pr√≥xima √©poca
        self.batch_metrics = []
    
    def on_train_end(self, trainer):
        """Ejecutado al final del entrenamiento"""
        print("\nüíæ Entrenamiento completado!")
        print(f"üìÇ Resultados en: {trainer.save_dir}")
